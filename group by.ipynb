{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "8f74d355-51b2-4161-859e-b15911e17776",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-------------+\n|   category|product_count|\n+-----------+-------------+\n|Electronics|            2|\n|   Clothing|            2|\n|  Furniture|            1|\n+-----------+-------------+\n\n"
     ]
    }
   ],
   "source": [
    "# Sample data for categories and products\n",
    "#Count the number of products in each category\n",
    "from pyspark.sql.functions import count\n",
    "data_category = [\n",
    "    (\"Electronics\", \"Laptop\"),\n",
    "    (\"Electronics\", \"Phone\"),\n",
    "    (\"Clothing\", \"T-Shirt\"),\n",
    "    (\"Clothing\", \"Jeans\"),\n",
    "    (\"Furniture\", \"Chair\"),\n",
    "]\n",
    "\n",
    "columns_category = [\"category\", \"product\"]\n",
    "\n",
    "# Creating the first DataFrame\n",
    "df_category = spark.createDataFrame(data_category, columns_category)\n",
    "df_category_c = df_category.groupby(\"category\").agg(count(\"product\").alias(\"product_count\"))\n",
    "df_category_c.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "90c2f9f3-cd68-4cfb-a6ba-1cc2be3e217f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----+\n|product|price|\n+-------+-----+\n| Laptop| 1000|\n|  Phone|  500|\n|T-Shirt|   20|\n|  Jeans|   50|\n|  Chair|  150|\n|  Chair|   15|\n+-------+-----+\n\n+-------+---------+--------+---------+\n|product|min_price|mx_price|avg_price|\n+-------+---------+--------+---------+\n| Laptop|     1000|    1000|   1000.0|\n|  Phone|      500|     500|    500.0|\n|T-Shirt|       20|      20|     20.0|\n|  Jeans|       50|      50|     50.0|\n|  Chair|       15|     150|     82.5|\n+-------+---------+--------+---------+\n\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#Calculate the minimum, maximum, and average price for each product\n",
    "from pyspark.sql.functions import count,min,max,avg\n",
    "data_price = [\n",
    "    (\"Laptop\", 1000),\n",
    "    (\"Phone\", 500),\n",
    "    (\"T-Shirt\", 20),\n",
    "    (\"Jeans\", 50),\n",
    "    (\"Chair\", 150),\n",
    "    (\"Chair\", 15)\n",
    "]\n",
    "\n",
    "columns_price = [\"product\", \"price\"]\n",
    "\n",
    "# Creating the second DataFrame\n",
    "df_price = spark.createDataFrame(data_price, columns_price)\n",
    "df_price_agg = df_price.groupby(\"product\").agg(min(\"price\").alias(\"min_price\")\n",
    "                                          ,max(\"price\").alias(\"mx_price\")\n",
    "                                          ,avg(\"price\").alias(\"avg_price\"))\n",
    "df_price.show()\n",
    "df_price_agg.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "9e64cea8-eab2-4c4b-8c14-5572d3e12cc1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+-----------+\n|Year|Month|sum amounth|\n+----+-----+-----------+\n|2023|    1|        100|\n|2023|    2|        200|\n|2023|    3|        300|\n|2023|    4|        400|\n|2023|    5|        500|\n+----+-----+-----------+\n\n+----------+--------+------+\n|order_date|    city|amount|\n+----------+--------+------+\n|2023-01-01|New York|   100|\n|2023-02-15|  London|   200|\n|2023-03-10|   Paris|   300|\n|2023-04-20|  Berlin|   400|\n|2023-05-05|   Tokyo|   500|\n+----------+--------+------+\n\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#Group sales by month and year, and calculate the total amount for each month-year\n",
    "combination.\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import year, month, sum\n",
    "\n",
    "# Initialize Spark session\n",
    "spark = SparkSession.builder.appName(\"SalesAnalysis\").getOrCreate()\n",
    "\n",
    "# Sample data\n",
    "sales_data = [\n",
    "    (\"2023-01-01\", \"New York\", 100),\n",
    "    (\"2023-02-15\", \"London\", 200),\n",
    "    (\"2023-03-10\", \"Paris\", 300),\n",
    "    (\"2023-04-20\", \"Berlin\", 400),\n",
    "    (\"2023-05-05\", \"Tokyo\", 500),\n",
    "]\n",
    "\n",
    "columns = [\"order_date\", \"city\", \"amount\"]\n",
    "\n",
    "# Create DataFrame\n",
    "df_sales = spark.createDataFrame(sales_data, columns)\n",
    "df_sales_month_year = df_sales.withColumn(\"Year\",year(\"order_date\")).withColumn(\"Month\",month(\"order_date\"))\n",
    "df_sales_month_year_agg = df_sales_month_year.groupby(\"Year\",\"Month\").agg(sum(\"amount\").alias(\"sum amounth\"))\n",
    "df_sales_month_year_agg.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "ed7a7a44-3bff-416e-9d90-f32ebcf1a06a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---+\n|product|sum|\n+-------+---+\n|  Jeans|  4|\n|T-Shirt|  3|\n| Laptop|  2|\n+-------+---+\n\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#Find the top 5 products (by total quantity sold) across all orders\n",
    "from pyspark.sql.functions import col, sum\n",
    "\n",
    "# Sample data\n",
    "product_data = [\n",
    "    (\"Laptop\", \"order_1\", 2),\n",
    "    (\"Phone\", \"order_2\", 1),\n",
    "    (\"T-Shirt\", \"order_1\", 3),\n",
    "    (\"Jeans\", \"order_3\", 4),\n",
    "    (\"Chair\", \"order_2\", 2),\n",
    "]\n",
    "\n",
    "columns = [\"product\", \"order_id\", \"quantity\"]\n",
    "\n",
    "# Create DataFrame\n",
    "df_products = spark.createDataFrame(product_data, columns)\n",
    "df_products_agg_top_3 = (\n",
    "    df_products.groupBy(\"product\")\n",
    "    .agg(sum(\"quantity\").alias(\"sum\"))\n",
    "    .orderBy(col(\"sum\").desc())\n",
    "    .limit(3)\n",
    ")\n",
    "\n",
    "# Show results\n",
    "df_products_agg_top_3.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "e6e73ae2-f1e4-4639-9054-4ef5a71e4461",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----+\n|user_id|AVG R|\n+-------+-----+\n|      1|  4.5|\n|      2|  3.5|\n|      3|  5.0|\n+-------+-----+\n\n+-------+----------+------+\n|user_id|product_id|rating|\n+-------+----------+------+\n|      1|         1|     4|\n|      1|         2|     5|\n|      2|         1|     3|\n|      2|         3|     4|\n|      3|         2|     5|\n+-------+----------+------+\n\n"
     ]
    }
   ],
   "source": [
    "# calculate the average rating given by each user\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import avg\n",
    "\n",
    "# Initialize Spark session\n",
    "spark = SparkSession.builder.appName(\"AverageRatingPerUser\").getOrCreate()\n",
    "\n",
    "# Sample data\n",
    "rating_data = [\n",
    "    (1, 1, 4),\n",
    "    (1, 2, 5),\n",
    "    (2, 1, 3),\n",
    "    (2, 3, 4),\n",
    "    (3, 2, 5),\n",
    "]\n",
    "\n",
    "columns = [\"user_id\", \"product_id\", \"rating\"]\n",
    "\n",
    "# Create DataFrame\n",
    "df_ratings = spark.createDataFrame(rating_data, columns)\n",
    "df_ratings_agg = df_ratings.groupBy(\"user_id\").agg(avg(\"rating\").alias(\"AVG R\"))\n",
    "df_ratings_agg.show()\n",
    "df_ratings.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "1bab60b2-4d44-4712-bf58-5b399c375595",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----------+\n|country|total_spent|\n+-------+-----------+\n|    USA|        300|\n|     UK|        150|\n| France|        550|\n+-------+-----------+\n\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#Group customers by country and calculate the total amount spent by customers in each country\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import sum\n",
    "\n",
    "# Initialize Spark session\n",
    "spark = SparkSession.builder.appName(\"TotalSpendByCountry\").getOrCreate()\n",
    "\n",
    "# Sample data\n",
    "customer_data = [\n",
    "    (1, \"USA\", \"order_1\", 100),\n",
    "    (1, \"USA\", \"order_2\", 200),\n",
    "    (2, \"UK\", \"order_3\", 150),\n",
    "    (3, \"France\", \"order_4\", 250),\n",
    "    (3, \"France\", \"order_5\", 300),\n",
    "]\n",
    "\n",
    "columns = [\"customer_id\", \"country\", \"order_id\", \"amount\"]\n",
    "\n",
    "# Create DataFrame\n",
    "df_customers = spark.createDataFrame(customer_data, columns)\n",
    "\n",
    "# Group by country and calculate total amount spent\n",
    "df_total_spend = df_customers.groupBy(\"country\").agg(sum(\"amount\").alias(\"total_spent\"))\n",
    "\n",
    "# Show results\n",
    "df_total_spend.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "b338d587-7498-40f7-86fb-3e45dd6fb630",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----------+\n|product|order_date|\n+-------+----------+\n|  Phone|2023-02-15|\n|T-Shirt|2023-03-10|\n+-------+----------+\n\n+-------+----------+\n|product|order_date|\n+-------+----------+\n| Laptop|2023-01-01|\n|  Jeans|2023-04-20|\n+-------+----------+\n\n"
     ]
    }
   ],
   "source": [
    "#Identify products that had no sales between 2023-02-01 and 2023-03-31.\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "# Initialize Spark session\n",
    "spark = SparkSession.builder.appName(\"NoSalesProducts\").getOrCreate()\n",
    "\n",
    "# Sample data\n",
    "sales_data = [\n",
    "    (\"Laptop\", \"2023-01-01\"),\n",
    "    (\"Phone\", \"2023-02-15\"),\n",
    "    (\"T-Shirt\", \"2023-03-10\"),\n",
    "    (\"Jeans\", \"2023-04-20\"),\n",
    "]\n",
    "\n",
    "columns = [\"product\", \"order_date\"]\n",
    "\n",
    "# Create DataFrame\n",
    "df_sales = spark.createDataFrame(sales_data, columns)\n",
    "\n",
    "# Filter for sales within the date range\n",
    "df_sales_in_range = df_sales.filter((col(\"order_date\") >= \"2023-02-01\") & (col(\"order_date\") <= \"2023-03-31\"))\n",
    "df_sales_in_range.show()\n",
    "# Find products with no sales in the specified range\n",
    "df_no_sales = df_sales.join(df_sales_in_range, \"product\", \"left_anti\")\n",
    "# Show results\n",
    "df_no_sales.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "c26689de-e547-4d50-8baf-e3f4e1ff5833",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------+-----------+\n|customer_id|    city|order_count|\n+-----------+--------+-----------+\n|          1|New York|          2|\n|          2|  London|          1|\n|          3|   Paris|          1|\n+-----------+--------+-----------+\n\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#Calculate Order Count per Customer and City\n",
    "from pyspark.sql.functions import count\n",
    "\n",
    "# Sample data\n",
    "customer_data = [\n",
    "    (1, \"New York\", \"order_1\"),\n",
    "    (1, \"New York\", \"order_2\"),\n",
    "    (2, \"London\", \"order_3\"),\n",
    "    (3, \"Paris\", \"order_4\"),\n",
    "]\n",
    "\n",
    "columns = [\"customer_id\", \"city\", \"order_id\"]\n",
    "\n",
    "# Create DataFrame\n",
    "df_customers = spark.createDataFrame(customer_data, columns)\n",
    "\n",
    "# Group by customer and city, and count the number of orders\n",
    "df_order_count = df_customers.groupBy(\"customer_id\", \"city\").agg(count(\"order_id\").alias(\"order_count\"))\n",
    "\n",
    "# Show results\n",
    "df_order_count.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "b3c9ca3b-91a7-4948-b00f-3b3180184b7a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+---------------+\n|day_type|avg_order_value|\n+--------+---------------+\n| Weekday|          300.0|\n| Weekend|          650.0|\n+--------+---------------+\n\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#Group Orders by Weekday and Calculate Average Order Value (when-otherwise)\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import dayofweek, when, avg, col\n",
    "\n",
    "# Initialize Spark session\n",
    "spark = SparkSession.builder.appName(\"WeekdayWeekendAverageOrderValue\").getOrCreate()\n",
    "\n",
    "# Sample data\n",
    "order_data = [\n",
    "    (\"2023-04-10\", 1, 100),  # Monday\n",
    "    (\"2023-04-11\", 2, 200),  # Tuesday\n",
    "    (\"2023-04-12\", 3, 300),  # Wednesday\n",
    "    (\"2023-04-13\", 1, 400),  # Thursday\n",
    "    (\"2023-04-14\", 2, 500),  # Friday\n",
    "    (\"2023-04-15\", 3, 600),  # Saturday (Weekend)\n",
    "    (\"2023-04-16\", 1, 700),  # Sunday (Weekend)\n",
    "]\n",
    "\n",
    "columns = [\"order_date\", \"customer_id\", \"amount\"]\n",
    "\n",
    "# Create DataFrame\n",
    "df_orders = spark.createDataFrame(order_data, columns)\n",
    "\n",
    "# Add a weekday number column (1 = Sunday, ..., 7 = Saturday)\n",
    "df_orders = df_orders.withColumn(\"weekday_number\", dayofweek(\"order_date\"))\n",
    "\n",
    "# Use when-otherwise to classify as \"Weekday\" or \"Weekend\"\n",
    "df_orders = df_orders.withColumn(\n",
    "    \"day_type\",\n",
    "    when(col(\"weekday_number\").between(2, 6), \"Weekday\")  # Monday to Friday\n",
    "    .otherwise(\"Weekend\")  # Saturday and Sunday\n",
    ")\n",
    "\n",
    "# Group by \"day_type\" and calculate the average order value\n",
    "df_avg_order_value = df_orders.groupBy(\"day_type\").agg(avg(\"amount\").alias(\"avg_order_value\"))\n",
    "\n",
    "# Show results\n",
    "df_avg_order_value.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "f6f5af5f-db44-4843-9366-63e357ea8f6a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------+\n| category|avg_price|\n+---------+---------+\n| Clothing|     20.0|\n|Furniture|    150.0|\n+---------+---------+\n\n"
     ]
    }
   ],
   "source": [
    "# Filter Products Starting with \"T\" and Group by Category with Average Price\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, avg\n",
    "\n",
    "# Initialize Spark session\n",
    "spark = SparkSession.builder.appName(\"FilteredProductCategoryAveragePrice\").getOrCreate()\n",
    "\n",
    "# Sample data\n",
    "product_data = [\n",
    "    (\"T-Shirt\", \"Clothing\", 20),\n",
    "    (\"Table\", \"Furniture\", 150),\n",
    "    (\"Jeans\", \"Clothing\", 50),\n",
    "    (\"Chair\", \"Furniture\", 100),\n",
    "]\n",
    "\n",
    "columns = [\"product\", \"category\", \"price\"]\n",
    "\n",
    "# Create DataFrame\n",
    "df_products = spark.createDataFrame(product_data, columns)\n",
    "\n",
    "# Filter products starting with \"T\" and group by category\n",
    "df_filtered_avg_price = df_products.filter(col(\"product\").startswith(\"T\")) \\\n",
    "                                   .groupBy(\"category\") \\\n",
    "                                   .agg(avg(\"price\").alias(\"avg_price\"))\n",
    "\n",
    "# Show results\n",
    "df_filtered_avg_price.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "dd19b92d-a56c-4551-aa46-c260d8e76b67",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-----+\n|customer_id|total|\n+-----------+-----+\n|          1|  250|\n|          2|  250|\n+-----------+-----+\n\n"
     ]
    }
   ],
   "source": [
    "#Group customers by customer ID and calculate the total amount spent. Filter customers who spent more than $200 in total.\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, sum\n",
    "\n",
    "# Initialize Spark session\n",
    "spark = SparkSession.builder.appName(\"CustomerTotalSpend\").getOrCreate()\n",
    "\n",
    "# Sample data\n",
    "order_data = [\n",
    "    (1, \"order_1\", 100),\n",
    "    (1, \"order_2\", 150),\n",
    "    (2, \"order_3\", 250),\n",
    "    (3, \"order_4\", 100),\n",
    "    (3, \"order_5\", 120),\n",
    "]\n",
    "\n",
    "columns = [\"customer_id\", \"order_id\", \"amount\"]\n",
    "\n",
    "# Create DataFrame\n",
    "df_orders = spark.createDataFrame(order_data, columns)\n",
    "total_200_g  = df_orders.groupBy(\"customer_id\").agg(sum(\"amount\").alias(\"total\")).filter(col(\"total\")>220)\n",
    "total_200_g.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "98895d2c-8eb2-49b3-b5df-fba6bc5395d1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+------+------------+\n|order_id|amount|order_status|\n+--------+------+------------+\n| order_1|   150|        High|\n| order_2|    80|         Low|\n| order_3|   220|        High|\n| order_4|    50|         Low|\n+--------+------+------------+\n\n"
     ]
    }
   ],
   "source": [
    "#Create a New Column with Order Status (\"High\" for > $100, \"Low\" Otherwise)\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, when\n",
    "\n",
    "# Initialize Spark session\n",
    "spark = SparkSession.builder.appName(\"OrderStatus\").getOrCreate()\n",
    "\n",
    "# Sample data\n",
    "order_data = [\n",
    "    (\"order_1\", 150),\n",
    "    (\"order_2\", 80),\n",
    "    (\"order_3\", 220),\n",
    "    (\"order_4\", 50),\n",
    "]\n",
    "\n",
    "columns = [\"order_id\", \"amount\"]\n",
    "\n",
    "# Create DataFrame\n",
    "df_orders = spark.createDataFrame(order_data, columns)\n",
    "\n",
    "# Create a new column \"order_status\" based on the \"amount\"\n",
    "df_orders_with_status = df_orders.withColumn(\n",
    "    \"order_status\",\n",
    "    when(col(\"amount\") > 100, \"High\").otherwise(\"Low\")\n",
    ")\n",
    "# Show results\n",
    "df_orders_with_status.show()\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "environmentMetadata": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 2
   },
   "notebookName": "group by",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
