{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "aaf8fa75-6d77-4757-8a08-64675e0284ed",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------+--------+\n|   name|last_checkin|  status|\n+-------+------------+--------+\n|Karthik|  2024-11-01|  Active|\n|   Neha|  2024-10-20|Inactive|\n|  Priya|  2024-10-28|  Active|\n|  Mohan|  2024-11-02|  Active|\n|   Ajay|  2024-09-15|Inactive|\n|  Vijay|  2024-10-30|  Active|\n|   Veer|  2024-10-25|Inactive|\n| Aatish|  2024-10-10|Inactive|\n|Animesh|  2024-10-15|Inactive|\n| Nishad|  2024-11-01|  Active|\n|  Varun|  2024-10-05|Inactive|\n|  Aadil|  2024-09-30|Inactive|\n+-------+------------+--------+\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, expr, current_date, when,datediff,count,avg\n",
    "\n",
    "# Create a Spark session\n",
    "spark = SparkSession.builder.appName(\"EmployeeStatus\").getOrCreate()\n",
    "\n",
    "# Employee data\n",
    "employees = [\n",
    "    (\"karthik\", \"2024-11-01\"),\n",
    "    (\"neha\", \"2024-10-20\"),\n",
    "    (\"priya\", \"2024-10-28\"),\n",
    "    (\"mohan\", \"2024-11-02\"),\n",
    "    (\"ajay\", \"2024-09-15\"),\n",
    "    (\"vijay\", \"2024-10-30\"),\n",
    "    (\"veer\", \"2024-10-25\"),\n",
    "    (\"aatish\", \"2024-10-10\"),\n",
    "    (\"animesh\", \"2024-10-15\"),\n",
    "    (\"nishad\", \"2024-11-01\"),\n",
    "    (\"varun\", \"2024-10-05\"),\n",
    "    (\"aadil\", \"2024-09-30\")\n",
    "]\n",
    "\n",
    "# Create DataFrame\n",
    "employees_df = spark.createDataFrame(employees, [\"name\", \"last_checkin\"])\n",
    "employees_df = employees_df.withColumn(\"last_checkin\", col(\"last_checkin\").cast(\"date\")) \\\n",
    "    .withColumn(\"name\", expr(\"initcap(name)\")) \\\n",
    "    .withColumn(\"status\", when(datediff(current_date(), col(\"last_checkin\")) <= 7, \"Active\").otherwise(\"Inactive\"))\n",
    "\n",
    "\n",
    "# Show the result\n",
    "employees_df.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "80d9f82b-9095-41f7-9cbe-f43c0e18d1dc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----------+------------------+\n|   name|total_sales|performance_status|\n+-------+-----------+------------------+\n|Karthik|      60000|         Excellent|\n|   Neha|      48000|              Good|\n|  Priya|      30000|              Good|\n|  Mohan|      24000| Needs Improvement|\n|   Ajay|      52000|         Excellent|\n|  Vijay|      45000|              Good|\n|   Veer|      70000|         Excellent|\n| Aatish|      23000| Needs Improvement|\n|Animesh|      15000| Needs Improvement|\n| Nishad|       8000| Needs Improvement|\n|  Varun|      29000|              Good|\n|  Aadil|      32000|              Good|\n+-------+-----------+------------------+\n\n+------------------+-----------+\n|performance_status|total_sales|\n+------------------+-----------+\n|         Excellent|     182000|\n|              Good|     184000|\n| Needs Improvement|      70000|\n+------------------+-----------+\n\n"
     ]
    }
   ],
   "source": [
    "\n",
    "sales = [\n",
    "(\"karthik\", 60000),\n",
    "(\"neha\", 48000),\n",
    "(\"priya\", 30000),\n",
    "(\"mohan\", 24000),\n",
    "(\"ajay\", 52000),\n",
    "(\"vijay\", 45000),\n",
    "(\"veer\", 70000),\n",
    "(\"aatish\", 23000),\n",
    "(\"animesh\", 15000),\n",
    "(\"nishad\", 8000),\n",
    "(\"varun\", 29000),\n",
    "(\"aadil\", 32000)\n",
    "]\n",
    "sales_df = spark.createDataFrame(sales, [\"name\", \"total_sales\"])\n",
    "# Capitalize name, determine performance status, and aggregate total sales by performance status\n",
    "sales_df = sales_df.withColumn(\"name\", expr(\"initcap(name)\")) \\\n",
    "    .withColumn(\"performance_status\", when(col(\"total_sales\") > 50000, \"Excellent\")\n",
    "                .when((col(\"total_sales\") > 25000) & (col(\"total_sales\") <= 50000), \"Good\")\n",
    "                .otherwise(\"Needs Improvement\"))\n",
    "    \n",
    "sales_df.show()\n",
    "    \n",
    "# Aggregate total sales by performance status\n",
    "aggregated_df = sales_df.groupBy(\"performance_status\").sum(\"total_sales\").withColumnRenamed(\"sum(total_sales)\", \"total_sales\")\n",
    "\n",
    "# Show the result\n",
    "aggregated_df.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "4c031ce4-2ffe-468f-b330-8200ad9f50e6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------+-----+--------------+\n|   name| project|hours|WorkLoad_level|\n+-------+--------+-----+--------------+\n|Karthik|ProjectA|  120|      Balanced|\n|Karthik|ProjectB|  100| Underutilized|\n|   Neha|ProjectC|   80| Underutilized|\n|   Neha|ProjectD|   30| Underutilized|\n|  Priya|ProjectE|  110|      Balanced|\n|  Mohan|ProjectF|   40| Underutilized|\n|   Ajay|ProjectG|   70| Underutilized|\n|  Vijay|ProjectH|  150|      Balanced|\n|   Veer|ProjectI|  190|      Balanced|\n| Aatish|ProjectJ|   60| Underutilized|\n|Animesh|ProjectK|   95| Underutilized|\n| Nishad|ProjectL|  210|    Overloaded|\n|  Varun|ProjectM|   50| Underutilized|\n|  Aadil|ProjectN|   90| Underutilized|\n+-------+--------+-----+--------------+\n\n+--------------+-----------+\n|WorkLoad_level|total_hours|\n+--------------+-----------+\n|      Balanced|        570|\n| Underutilized|        615|\n|    Overloaded|        210|\n+--------------+-----------+\n\n"
     ]
    }
   ],
   "source": [
    "workload = [\n",
    "(\"karthik\", \"ProjectA\", 120),\n",
    "(\"karthik\", \"ProjectB\", 100),\n",
    "(\"neha\", \"ProjectC\", 80),\n",
    "(\"neha\", \"ProjectD\", 30),\n",
    "(\"priya\", \"ProjectE\", 110),\n",
    "(\"mohan\", \"ProjectF\", 40),\n",
    "(\"ajay\", \"ProjectG\", 70),\n",
    "(\"vijay\", \"ProjectH\", 150),\n",
    "(\"veer\", \"ProjectI\", 190),\n",
    "(\"aatish\", \"ProjectJ\", 60),\n",
    "(\"animesh\", \"ProjectK\", 95),\n",
    "(\"nishad\", \"ProjectL\", 210),\n",
    "(\"varun\", \"ProjectM\", 50),\n",
    "(\"aadil\", \"ProjectN\", 90)\n",
    "]\n",
    "workload_df = spark.createDataFrame(workload, [\"name\", \"project\", \"hours\"])\n",
    "workload_df = workload_df.withColumn(\"name\" ,expr(\"initcap(name)\"))\\\n",
    "               .withColumn(\"WorkLoad_level\",when(col(\"hours\")>200,\"Overloaded\")\n",
    "               .when((col(\"hours\") > 100) & (col(\"hours\") <= 200), \"Balanced\")\n",
    "               .otherwise(\"Underutilized\"))\n",
    "workload_df_agg = workload_df.groupby(\"WorkLoad_level\").sum(\"hours\").withColumnRenamed(\"sum(hours)\", \"total_hours\")\n",
    "workload_df.show()\n",
    "workload_df_agg.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "340f3f96-300d-4ac5-850f-ea3ba80c7bcf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+-----+\n|   overtime_status|count|\n+------------------+-----+\n|Excessive Overtime|    3|\n| Standard Overtime|    4|\n|       No Overtime|    3|\n+------------------+-----+\n\n+-------+------------+------------------+\n|   name|hours_worked|   overtime_status|\n+-------+------------+------------------+\n|Karthik|          62|Excessive Overtime|\n|   Neha|          50| Standard Overtime|\n|  Priya|          30|       No Overtime|\n|  Mohan|          65|Excessive Overtime|\n|   Ajay|          40|       No Overtime|\n|  Vijay|          47| Standard Overtime|\n|   Veer|          55| Standard Overtime|\n| Aatish|          30|       No Overtime|\n|Animesh|          75|Excessive Overtime|\n| Nishad|          60| Standard Overtime|\n+-------+------------+------------------+\n\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Determine whether an employee has \"Excessive Overtime\" if their weekly hours exceed 60,\n",
    "\"Standard Overtime\" if between 45-60 hours, and \"No Overtime\" if below 45 hours. Capitalize each\n",
    "name and group by overtime status.\"\"\"\n",
    "\n",
    "\n",
    "employees = [\n",
    "(\"karthik\", 62),\n",
    "(\"neha\", 50),\n",
    "(\"priya\", 30),\n",
    "(\"mohan\", 65),\n",
    "(\"ajay\", 40),\n",
    "(\"vijay\", 47),\n",
    "(\"veer\", 55),\n",
    "(\"aatish\", 30),\n",
    "(\"animesh\", 75),\n",
    "(\"nishad\", 60)\n",
    "]\n",
    "employees_df = spark.createDataFrame(employees, [\"name\", \"hours_worked\"])\n",
    "# Apply overtime classification and capitalize names\n",
    "employees_df = employees_df.withColumn(\n",
    "    \"overtime_status\",\n",
    "    when(col(\"hours_worked\") > 60, \"Excessive Overtime\")\n",
    "    .when((col(\"hours_worked\") >= 45) & (col(\"hours_worked\") <= 60), \"Standard Overtime\")\n",
    "    .otherwise(\"No Overtime\")\n",
    ").withColumn(\"name\", expr(\"initcap(name)\"))\n",
    "\n",
    "# Group by overtime status and show results\n",
    "employees_df.groupBy(\"overtime_status\").count().show()\n",
    "\n",
    "employees_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "57a2be4e-296f-4591-bd21-29eaf53f8bc0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-------+-------------------+\n|vehicle_name|mileage|         Efficiency|\n+------------+-------+-------------------+\n|        CarA|     30|    High Efficiency|\n|        CarB|     22|Moderate Efficiency|\n|        CarC|     18|Moderate Efficiency|\n|        CarD|     15|Moderate Efficiency|\n|        CarE|     10|     Low Efficiency|\n|        CarF|     28|    High Efficiency|\n|        CarG|     12|     Low Efficiency|\n|        CarH|     35|    High Efficiency|\n|        CarI|     25|Moderate Efficiency|\n|        CarJ|     16|Moderate Efficiency|\n+------------+-------+-------------------+\n\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\"\"\"Classify each vehicleâ€™s mileage as \"High Efficiency\" if mileage is above 25 MPG, \"Moderate Efficiency\"\n",
    "if between 15-25 MPG, and \"Low Efficiency\" if below 15 MPG.\"\"\"\n",
    "\n",
    "vehicles = [\n",
    "(\"CarA\", 30),\n",
    "(\"CarB\", 22),\n",
    "(\"CarC\", 18),\n",
    "(\"CarD\", 15),\n",
    "(\"CarE\", 10),\n",
    "(\"CarF\", 28),\n",
    "(\"CarG\", 12),\n",
    "(\"CarH\", 35),\n",
    "(\"CarI\", 25),\n",
    "(\"CarJ\", 16)\n",
    "]\n",
    "vehicles_df = spark.createDataFrame(vehicles, [\"vehicle_name\", \"mileage\"])\n",
    "vehicles_df = vehicles_df.withColumn(\"Efficiency\",when(col(\"mileage\")>25,\"High Efficiency\")\n",
    "                                     .when((col(\"mileage\") >= 15) & (col(\"mileage\") <= 25), \"Moderate Efficiency\")\n",
    "                                     .otherwise(\"Low Efficiency\")\n",
    "                                     )\n",
    "vehicles_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "413f9c72-bf5f-4ce2-a10e-c927877259f2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----+-----------------+\n|   name|score|         Classify|\n+-------+-----+-----------------+\n|Karthik|   95|        Excellent|\n|   Neha|   82|             GOOD|\n|  Priya|   74|Needs Improvement|\n|  Mohan|   91|        Excellent|\n|   Ajay|   67|Needs Improvement|\n|  Vijay|   80|             GOOD|\n|   Veer|   85|             GOOD|\n| Aatish|   72|Needs Improvement|\n|Animesh|   90|        Excellent|\n| Nishad|   60|Needs Improvement|\n+-------+-----+-----------------+\n\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\"\"\"Classify students based on their scores as \"Excellent\" if score is 90 or above, \"Good\" if between 75-\n",
    "89, and \"Needs Improvement\" if below 75. Count students in each category.\"\"\"\n",
    "\n",
    "students = [\n",
    "(\"karthik\", 95),\n",
    "(\"neha\", 82),\n",
    "(\"priya\", 74),\n",
    "(\"mohan\", 91),\n",
    "(\"ajay\", 67),\n",
    "(\"vijay\", 80),\n",
    "(\"veer\", 85),\n",
    "(\"aatish\", 72),\n",
    "(\"animesh\", 90),\n",
    "(\"nishad\", 60)\n",
    "]\n",
    "students_df = spark.createDataFrame(students, [\"name\", \"score\"])\n",
    "students_df = students_df.withColumn(\"Classify\",when(col(\"score\")>=90,\"Excellent\")\n",
    "                         .when((col(\"score\")>=75) & (col(\"score\") <= 89) ,\"GOOD\")\n",
    "                         .otherwise(\"Needs Improvement\"))\\\n",
    "                        .withColumn(\"name\",expr(\"initcap(name)\"))\n",
    "students_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "723a99ee-3835-486a-9d94-9e4efcf2f597",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+--------------+-----------+\n|product_name|stock_quantity|   Classify|\n+------------+--------------+-----------+\n|    ProductA|           120|Overstocked|\n|    ProductB|            95|     Normal|\n|    ProductC|            45|  Low Stock|\n|    ProductD|           200|Overstocked|\n|    ProductE|            75|     Normal|\n|    ProductF|            30|  Low Stock|\n|    ProductG|            85|     Normal|\n|    ProductH|           100|     Normal|\n|    ProductI|            60|     Normal|\n|    ProductJ|            20|  Low Stock|\n+------------+--------------+-----------+\n\n+-----------+--------------+\n|   Classify|Total Quantity|\n+-----------+--------------+\n|Overstocked|           320|\n|     Normal|           415|\n|  Low Stock|            95|\n+-----------+--------------+\n\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Classify inventory stock levels as \"Overstocked\" if stock exceeds 100, \"Normal\" if between 50-100,\n",
    "and \"Low Stock\" if below 50. Aggregate total stock in each category.\"\"\"\n",
    "\n",
    "\n",
    "inventory = [\n",
    "(\"ProductA\", 120),\n",
    "(\"ProductB\", 95),\n",
    "(\"ProductC\", 45),\n",
    "(\"ProductD\", 200),\n",
    "(\"ProductE\", 75),\n",
    "(\"ProductF\", 30),\n",
    "(\"ProductG\", 85),\n",
    "(\"ProductH\", 100),\n",
    "(\"ProductI\", 60),\n",
    "(\"ProductJ\", 20)\n",
    "]\n",
    "inventory_df = spark.createDataFrame(inventory, [\"product_name\", \"stock_quantity\"])\n",
    "inventory_df = inventory_df.withColumn(\"Classify\",when(col(\"stock_quantity\")>100,\"Overstocked\")\n",
    "                            .when((col(\"stock_quantity\")>=50) & (col(\"stock_quantity\")<=100),\"Normal\")\n",
    "                            .otherwise(\"Low Stock\"))\n",
    "inventory_df_agg = inventory_df.groupby(\"Classify\").sum(\"stock_quantity\").withColumnRenamed(\"sum(stock_quantity)\",\"Total Quantity\")\n",
    "inventory_df.show()\n",
    "inventory_df_agg.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "a4e7a438-b13c-4ff3-ad96-f7d3f7371a29",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----------+\n|department|total_bonus|\n+----------+-----------+\n|     Sales|    40000.0|\n| Marketing|    20000.0|\n|        IT|    15000.0|\n|   Finance|    15000.0|\n|        HR|    15000.0|\n+----------+-----------+\n\n+-------+----------+-----------------+-------+\n|   name|department|performance_score|  bonus|\n+-------+----------+-----------------+-------+\n|karthik|     Sales|               85|20000.0|\n|   neha| Marketing|               78|    0.0|\n|  priya|        IT|               90|15000.0|\n|  mohan|   Finance|               65|    0.0|\n|   ajay|     Sales|               55|    0.0|\n|  vijay| Marketing|               82|20000.0|\n|   veer|        HR|               72|15000.0|\n| aatish|     Sales|               88|20000.0|\n|animesh|   Finance|               95|15000.0|\n| nishad|        IT|               60|    0.0|\n+-------+----------+-----------------+-------+\n\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\"\"\"Classify employees for a bonus eligibility program. Employees in \"Sales\" and \"Marketing\" with\n",
    "performance scores above 80 get a 20% bonus, while others with scores above 70 get 15%. All other\n",
    "employees receive no bonus. Group by department and calculate total bonus allocation.\"\"\"\n",
    "\n",
    "employees = [\n",
    "(\"karthik\", \"Sales\", 85),\n",
    "(\"neha\", \"Marketing\", 78),\n",
    "(\"priya\", \"IT\", 90),\n",
    "(\"mohan\", \"Finance\", 65),\n",
    "(\"ajay\", \"Sales\", 55),\n",
    "(\"vijay\", \"Marketing\", 82),\n",
    "(\"veer\", \"HR\", 72),\n",
    "(\"aatish\", \"Sales\", 88),\n",
    "(\"animesh\", \"Finance\", 95),\n",
    "(\"nishad\", \"IT\", 60)\n",
    "]\n",
    "employees_df = spark.createDataFrame(employees, [\"name\", \"department\", \"performance_score\"])\n",
    "base_salary = 100000\n",
    "\n",
    "# Create DataFrame\n",
    "employees_df = spark.createDataFrame(employees, [\"name\", \"department\", \"performance_score\"])\n",
    "\n",
    "# Calculate bonus based on the eligibility criteria\n",
    "employees_df = employees_df.withColumn(\n",
    "    \"bonus\",\n",
    "    when((col(\"department\").isin(\"Sales\", \"Marketing\")) & (col(\"performance_score\") > 80), 0.20 * base_salary)\n",
    "    .when((~col(\"department\").isin(\"Sales\", \"Marketing\")) & (col(\"performance_score\") > 70), 0.15 * base_salary)\n",
    "    .otherwise(0)\n",
    ")\n",
    "\n",
    "# Group by department and calculate total bonus allocation\n",
    "bonus_allocation_df = employees_df.groupBy(\"department\").sum(\"bonus\").withColumnRenamed(\"sum(bonus)\", \"total_bonus\")\n",
    "\n",
    "# Show the result\n",
    "bonus_allocation_df.show()\n",
    "employees_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "62113d42-dccb-41b5-8017-c66e40714ea2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----------+-----+\n|            classify|   category|count|\n+--------------------+-----------+-----+\n|    High Return Rate|Accessories|    1|\n|Moderate Return Rate|Accessories|    1|\n|Moderate Return Rate|   Clothing|    1|\n|     Low Return Rate|   Clothing|    2|\n|    High Return Rate|Electronics|    2|\n|Moderate Return Rate|Electronics|    2|\n|     Low Return Rate|Electronics|    1|\n+--------------------+-----------+-----+\n\n+------------+-----------+------------+------------------+--------------------+\n|product_name|   category|return_count|satisfaction_score|            classify|\n+------------+-----------+------------+------------------+--------------------+\n|      Laptop|Electronics|         120|                45|    High Return Rate|\n|  Smartphone|Electronics|          80|                60|Moderate Return Rate|\n|      Tablet|Electronics|          50|                72|     Low Return Rate|\n|  Headphones|Accessories|         110|                47|    High Return Rate|\n|       Shoes|   Clothing|          90|                55|Moderate Return Rate|\n|      Jacket|   Clothing|          30|                80|     Low Return Rate|\n|          TV|Electronics|         150|                40|    High Return Rate|\n|       Watch|Accessories|          60|                65|Moderate Return Rate|\n|       Pants|   Clothing|          25|                75|     Low Return Rate|\n|      Camera|Electronics|          95|                58|Moderate Return Rate|\n+------------+-----------+------------+------------------+--------------------+\n\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "For each product, classify return reasons as \"High Return Rate\" if return count exceeds 100 and\n",
    "satisfaction score below 50, \"Moderate Return Rate\" if return count is between 50-100 with a score\n",
    "between 50-70, and \"Low Return Rate\" otherwise. Group by category to count product return rates\"\"\"\n",
    "\n",
    "\n",
    "products = [\n",
    "(\"Laptop\", \"Electronics\", 120, 45),\n",
    "(\"Smartphone\", \"Electronics\", 80, 60),\n",
    "(\"Tablet\", \"Electronics\", 50, 72),\n",
    "(\"Headphones\", \"Accessories\", 110, 47),\n",
    "(\"Shoes\", \"Clothing\", 90, 55),\n",
    "(\"Jacket\", \"Clothing\", 30, 80),\n",
    "(\"TV\", \"Electronics\", 150, 40),\n",
    "(\"Watch\", \"Accessories\", 60, 65),\n",
    "(\"Pants\", \"Clothing\", 25, 75),\n",
    "(\"Camera\", \"Electronics\", 95, 58)\n",
    "]\n",
    "products_df = spark.createDataFrame(products, [\"product_name\", \"category\", \"return_count\",\n",
    "\"satisfaction_score\"])\n",
    "\n",
    "products_df = products_df.withColumn(\"classify\",when((col(\"return_count\")>100) & (col(\"satisfaction_score\")<50),\"High Return Rate\")\n",
    "                                     .when((col(\"return_count\").between(50, 100)) & (col(\"satisfaction_score\").between(50, 70)), \"Moderate Return Rate\")\n",
    "                                     .otherwise(\"Low Return Rate\"))\n",
    "\n",
    "products_df_agg = products_df.groupby(\"classify\",\"category\").count().orderBy(\"category\")\n",
    "products_df_agg.show()\n",
    "products_df.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "242eeaa3-8c65-484c-b794-7f5dc0f802d4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----------------+\n|membership|    avg(spending)|\n+----------+-----------------+\n|   Premium|           1062.5|\n|  Standard|            700.0|\n|     Basic|316.6666666666667|\n+----------+-----------------+\n\n+-------+----------+--------+---+---------------+\n|   name|membership|spending|age|       Classify|\n+-------+----------+--------+---+---------------+\n|karthik|   Premium|    1050| 32|   High Spender|\n|   neha|  Standard|     800| 28|Average Spender|\n|  priya|   Premium|    1200| 40|   High Spender|\n|  mohan|     Basic|     300| 35|    Low Spender|\n|   ajay|  Standard|     700| 25|Average Spender|\n|  vijay|   Premium|     500| 45|    Low Spender|\n|   veer|     Basic|     450| 33|    Low Spender|\n| aatish|  Standard|     600| 29|Average Spender|\n|animesh|   Premium|    1500| 60|   High Spender|\n| nishad|     Basic|     200| 21|    Low Spender|\n+-------+----------+--------+---+---------------+\n\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Classify customers' spending as \"High Spender\" if spending exceeds $1000 with \"Premium\"\n",
    "membership, \"Average Spender\" if spending between $500-$1000 and membership is \"Standard\",\n",
    "and \"Low Spender\" otherwise. Group by membership and calculate average spending.\"\"\"\n",
    "\n",
    "customers = [\n",
    "(\"karthik\", \"Premium\", 1050, 32),\n",
    "(\"neha\", \"Standard\", 800, 28),\n",
    "(\"priya\", \"Premium\", 1200, 40),\n",
    "(\"mohan\", \"Basic\", 300, 35),\n",
    "(\"ajay\", \"Standard\", 700, 25),\n",
    "(\"vijay\", \"Premium\", 500, 45),\n",
    "(\"veer\", \"Basic\", 450, 33),\n",
    "(\"aatish\", \"Standard\", 600, 29),\n",
    "(\"animesh\", \"Premium\", 1500, 60),\n",
    "(\"nishad\", \"Basic\", 200, 21)\n",
    "]\n",
    "customers_df = spark.createDataFrame(customers, [\"name\", \"membership\", \"spending\", \"age\"])\n",
    "customers_df = customers_df.withColumn(\"Classify\",when((col(\"spending\")>1000) & (col(\"membership\")== \"Premium\") ,\"High Spender\")\n",
    "                            .when((col(\"spending\").between(500,1000)) & (col(\"membership\")== \"Standard\"),\"Average Spender\")\n",
    "                            .otherwise(\"Low Spender\"))\n",
    "customers_df_agg = customers_df.groupby(\"membership\").avg(\"spending\").withcolumnrenamed(\"avg(spending)\", \"AVG SPENDING\")\n",
    "customers_df_agg.show()\n",
    "customers_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "87a40d6b-b0bb-487e-a66c-107c166f931b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+------------+-----+\n|Classify|product_type|count|\n+--------+------------+-----+\n|    FAST|      Laptop|    1|\n| Delayed|       Shoes|    2|\n| On-Time|  Smartphone|    1|\n| On-Time|       Watch|    1|\n| On-Time|      Tablet|    2|\n| Delayed|  Headphones|    1|\n|    FAST|      Camera|    1|\n| On-Time|      Laptop|    1|\n+--------+------------+-----+\n\n+--------+------------+-------------+-------------+--------+\n|order_id|product_type|       origin|delivery_days|Classify|\n+--------+------------+-------------+-------------+--------+\n|  Order1|      Laptop|     Domestic|            2|    FAST|\n|  Order2|       Shoes|International|            8| Delayed|\n|  Order3|  Smartphone|     Domestic|            3| On-Time|\n|  Order4|      Tablet|International|            5| On-Time|\n|  Order5|       Watch|     Domestic|            7| On-Time|\n|  Order6|  Headphones|International|           10| Delayed|\n|  Order7|      Camera|     Domestic|            1|    FAST|\n|  Order8|       Shoes|International|            9| Delayed|\n|  Order9|      Laptop|     Domestic|            6| On-Time|\n| Order10|      Tablet|International|            4| On-Time|\n+--------+------------+-------------+-------------+--------+\n\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Classify orders as \"Delayed\" if delivery time exceeds 7 days and origin location is \"International\",\n",
    "\"On-Time\" if between 3-7 days, and \"Fast\" if below 3 days. Group by product type to see the count of\n",
    "each delivery speed category\"\"\"\n",
    "\n",
    "orders = [\n",
    "(\"Order1\", \"Laptop\", \"Domestic\", 2),\n",
    "(\"Order2\", \"Shoes\", \"International\", 8),\n",
    "(\"Order3\", \"Smartphone\", \"Domestic\", 3),\n",
    "(\"Order4\", \"Tablet\", \"International\", 5),\n",
    "(\"Order5\", \"Watch\", \"Domestic\", 7),\n",
    "(\"Order6\", \"Headphones\", \"International\", 10),\n",
    "(\"Order7\", \"Camera\", \"Domestic\", 1),\n",
    "(\"Order8\", \"Shoes\", \"International\", 9),\n",
    "(\"Order9\", \"Laptop\", \"Domestic\", 6),\n",
    "(\"Order10\", \"Tablet\", \"International\", 4)\n",
    "]\n",
    "\n",
    "orders_df = spark.createDataFrame(orders, [\"order_id\", \"product_type\", \"origin\", \"delivery_days\"])\n",
    "orders_df = orders_df.withColumn(\"Classify\",when((col(\"delivery_days\")>7) & (col(\"origin\")==\"International\"),\"Delayed\")\n",
    "                                 .when(col(\"delivery_days\").between(3,7) ,\"On-Time\")\n",
    "                                 .otherwise(\"FAST\"))\n",
    "orders_df_agg = orders_df.groupby(\"Classify\",\"product_type\").count()  \n",
    "orders_df_agg.show()                               \n",
    "orders_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "761aac4a-7569-429a-95db-22c7865560e5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+-----+\n|   Risk_Level|count|\n+-------------+-----+\n|     Low Risk|    4|\n|Moderate Risk|    4|\n|    High Risk|    2|\n+-------------+-----+\n\n+------------+---------------+\n|Income_Range|avg_loan_amount|\n+------------+---------------+\n|        <50k|        90000.0|\n|     50-100k|       200000.0|\n+------------+---------------+\n\n+-------+------+-----------+------------+-------------+------------+\n|   name|income|loan_amount|credit_score|   Risk_Level|Income_Range|\n+-------+------+-----------+------------+-------------+------------+\n|karthik| 60000|     120000|         590|     Low Risk|     50-100k|\n|   neha| 90000|     180000|         610|Moderate Risk|     50-100k|\n|  priya| 50000|      75000|         680|Moderate Risk|     50-100k|\n|  mohan|120000|     240000|         560|     Low Risk|       >100k|\n|   ajay| 45000|      60000|         620|Moderate Risk|        <50k|\n|  vijay|100000|     100000|         700|     Low Risk|     50-100k|\n|   veer| 30000|      90000|         580|    High Risk|        <50k|\n| aatish| 85000|      85000|         710|     Low Risk|     50-100k|\n|animesh| 50000|     100000|         650|Moderate Risk|     50-100k|\n| nishad| 75000|     200000|         540|    High Risk|     50-100k|\n+-------+------+-----------+------------+-------------+------------+\n\n+------------+-------------+-----------------+\n|Income_Range|   Risk_Level| avg_credit_score|\n+------------+-------------+-----------------+\n|     50-100k|Moderate Risk|646.6666666666666|\n|        <50k|Moderate Risk|            620.0|\n|       >100k|     Low Risk|            560.0|\n|        <50k|    High Risk|            580.0|\n|     50-100k|    High Risk|            540.0|\n+------------+-------------+-----------------+\n\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Question Set:\n",
    "1. Classify loan applicants as \"High Risk\" if the loan amount exceeds twice their income and\n",
    "credit score is below 600, \"Moderate Risk\" if the loan amount is between 1-2 times their\n",
    "income and credit score between 600-700, and \"Low Risk\" otherwise. Find the total count of\n",
    "each risk level.\n",
    "2. For applicants classified as \"High Risk,\" calculate the average loan amount by income range\n",
    "(e.g., < 50k, 50-100k, >100k).\n",
    "3. Group by income brackets (<50k, 50-100k, >100k) and calculate the average credit score for\n",
    "each risk level. Filter for groups where average credit score is below 650.\"\"\"\n",
    "\n",
    "\n",
    "loan_applicants = [\n",
    "(\"karthik\", 60000, 120000, 590),\n",
    "(\"neha\", 90000, 180000, 610),\n",
    "(\"priya\", 50000, 75000, 680),\n",
    "(\"mohan\", 120000, 240000, 560),\n",
    "(\"ajay\", 45000, 60000, 620),\n",
    "(\"vijay\", 100000, 100000, 700),\n",
    "(\"veer\", 30000, 90000, 580),\n",
    "(\"aatish\", 85000, 85000, 710),\n",
    "(\"animesh\", 50000, 100000, 650),\n",
    "(\"nishad\", 75000, 200000, 540)\n",
    "]\n",
    "\n",
    "loan_applicants_df = spark.createDataFrame(loan_applicants, [\"name\", \"income\", \"loan_amount\",\n",
    "\"credit_score\"])\n",
    "loan_applicants_df = loan_applicants_df.withColumn(\"Risk_Level\",when((col(\"credit_score\")<600) & (col(\"loan_amount\")> 2*col(\"income\")),\"High Risk\")\n",
    "    .when((col(\"loan_amount\") > col(\"income\")) & (col(\"loan_amount\") <= 2 * col(\"income\")) & (col(\"credit_score\").between(600, 700)), \"Moderate Risk\")\n",
    "    .otherwise(\"Low Risk\"))\n",
    "loan_applicants_df = loan_applicants_df.withColumn(\n",
    "    \"Income_Range\",\n",
    "    when(col(\"income\") < 50000, \"<50k\")\n",
    "    .when((col(\"income\") >= 50000) & (col(\"income\") <= 100000), \"50-100k\")\n",
    "    .otherwise(\">100k\")\n",
    ")\n",
    "loan_applicants_df_agg = loan_applicants_df.groupby(\"Risk_Level\").count()\n",
    "loan_applicants_df_agg.show()\n",
    "high_risk_avg_loan_df = loan_applicants_df.filter(col(\"Risk_Level\") == \"High Risk\") \\\n",
    "    .groupBy(\"Income_Range\").agg(avg(\"loan_amount\").alias(\"avg_loan_amount\"))\n",
    "high_risk_avg_loan_df.show()\n",
    "loan_applicants_df.show()\n",
    "\n",
    "avg_credit_score_df = loan_applicants_df.groupBy(\"Income_Range\", \"Risk_Level\") \\\n",
    "    .agg(avg(\"credit_score\").alias(\"avg_credit_score\"))\n",
    "\n",
    "# Filter for groups where average credit score is below 650\n",
    "low_credit_score_df = avg_credit_score_df.filter(col(\"avg_credit_score\") < 650)\n",
    "low_credit_score_df.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "52d98d87-5bee-4b81-9648-850efa1562e8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------------------------+\n|membership|min(total_purchase_amount)|\n+----------+--------------------------+\n|   Premium|                      5000|\n|  Standard|                      3000|\n+----------+--------------------------+\n\n+----------+--------------------------+\n|membership|min(total_purchase_amount)|\n+----------+--------------------------+\n|   Premium|                      5000|\n|  Standard|                      3000|\n+----------+--------------------------+\n\n+----------+----------+--------------------------+\n|Categorize|membership|avg(total_purchase_amount)|\n+----------+----------+--------------------------+\n|  Frequent|   Premium|                    8000.0|\n+----------+----------+--------------------------+\n\n+-------+----------+------------------------+---------------------+----------+\n|   name|membership|days_since_last_purchase|total_purchase_amount|Categorize|\n+-------+----------+------------------------+---------------------+----------+\n|karthik|   Premium|                      50|                 5000|      Rare|\n|   neha|  Standard|                      10|                 2000|  Frequent|\n|  priya|   Premium|                      65|                 8000|Occasional|\n|  mohan|     Basic|                      90|                 1200|Occasional|\n|   ajay|  Standard|                      25|                 3500|  Frequent|\n|  vijay|   Premium|                      15|                 7000|  Frequent|\n|   veer|     Basic|                      75|                 1500|Occasional|\n| aatish|  Standard|                      45|                 3000|      Rare|\n|animesh|   Premium|                      20|                 9000|  Frequent|\n| nishad|     Basic|                      80|                 1100|Occasional|\n+-------+----------+------------------------+---------------------+----------+\n\n"
     ]
    }
   ],
   "source": [
    "\"\"\"4. Categorize customers based on purchase recency: \"Frequent\" if last purchase within\n",
    "30 days, \"Occasional\" if within 60 days, and \"Rare\" if over 60 days. Show the number of each\n",
    "category per membership type.\n",
    "5. Find the average total purchase amount for customers with \"Frequent\" purchase recency\n",
    "and \"Premium\" membership.\n",
    "6. For customers with \"Rare\" recency, calculate the minimum purchase amount across different\n",
    "membership types\"\"\"\n",
    "\n",
    "customer_purchases = [\n",
    "(\"karthik\", \"Premium\", 50, 5000),\n",
    "(\"neha\", \"Standard\", 10, 2000),\n",
    "(\"priya\", \"Premium\", 65, 8000),\n",
    "(\"mohan\", \"Basic\", 90, 1200),\n",
    "(\"ajay\", \"Standard\", 25, 3500),\n",
    "(\"vijay\", \"Premium\", 15, 7000),\n",
    "(\"veer\", \"Basic\", 75, 1500),\n",
    "(\"aatish\", \"Standard\", 45, 3000),\n",
    "(\"animesh\", \"Premium\", 20, 9000),\n",
    "(\"nishad\", \"Basic\", 80, 1100)\n",
    "]\n",
    "customer_purchases_df = spark.createDataFrame(customer_purchases, [\"name\", \"membership\",\n",
    "\"days_since_last_purchase\", \"total_purchase_amount\"])\n",
    "customer_purchases_df = customer_purchases_df.withColumn(\"Categorize\",when(col(\"days_since_last_purchase\")<30,\"Frequent\")\n",
    "                                                         .when(col(\"days_since_last_purchase\").between(30,60),\"Rare\")\n",
    "                                                         .otherwise(\"Occasional\"))\n",
    "avg_total_purchase = customer_purchases_df.groupBy(\"Categorize\", \"membership\") \\\n",
    "    .avg(\"total_purchase_amount\") \\\n",
    "    .filter((col(\"Categorize\") == \"Frequent\") & (col(\"membership\") == \"Premium\")) \n",
    "\n",
    "min_purchase = customer_purchases_df.filter(col(\"Categorize\") == \"Rare\") \\\n",
    "                                    .groupBy(\"membership\") \\\n",
    "                                    .min(\"total_purchase_amount\")\n",
    "\n",
    "# Show the result\n",
    "min_purchase.show()                                      \n",
    "avg_total_purchase.show()\n",
    "customer_purchases_df.show()"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "environmentMetadata": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "pyspark questions",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
