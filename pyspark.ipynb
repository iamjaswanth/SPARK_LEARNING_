{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "d96a3272-f167-4e07-8c96-b284bb2d2894",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "165000.0"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Task - Write a Spark code snippet to calculate the sum of a column in a DataFrame\n",
    "\"\"\"\n",
    "from pyspark.sql.functions import col,sum\n",
    "# Sample employee data\n",
    "data = [(\"John Doe\", \"john@example.com\", 50000.0),\n",
    "    (\"Jane Smith\", \"jane@example.com\", 60000.0),\n",
    "    (\"Bob Johnson\", \"bob@example.com\", 55000.0)]\n",
    "\n",
    "\n",
    "schema=\"Name string,email string,salary double\"\n",
    "df=spark.createDataFrame(data,schema)\n",
    "ds2 = df.agg(sum(\"salary\").alias(\"Total_salary\")).first()[0]\n",
    "display(ds2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "87aea2bc-04f6-42ba-99f1-3b88afd13512",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------+---------+----------+\n|ProductCode|Quantity|UnitPrice|CustomerID|\n+-----------+--------+---------+----------+\n|       P001|       5|     20.0|      C001|\n|       P002|       3|     15.5|      C002|\n|       P003|      10|     5.99|      C003|\n|       P004|       2|     50.0|      C001|\n+-----------+--------+---------+----------+\n\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\"\"\"\n",
    "Task - Identify rows containing non-numeric values in the \"Quantity\" column, if any.\n",
    "\"\"\"\n",
    "from pyspark.sql.types import *\n",
    "schema = StructType([\n",
    "  StructField(\"ProductCode\", StringType(), True),\n",
    "  StructField(\"Quantity\", StringType(), True),\n",
    "  StructField(\"UnitPrice\", StringType(), True),\n",
    "  StructField(\"CustomerID\", StringType(), True),\n",
    "])\n",
    " \n",
    "data = [\n",
    "  (\"P001\", 5, 20.0, \"C001\"),\n",
    "  (\"P002\", 3, 15.5, \"C002\"),\n",
    "  (\"P003\", 10, 5.99, \"C003\"),\n",
    "  (\"P004\", 2, 50.0, \"C001\"),\n",
    "  (\"P005\", \"eight\", 12.75, \"C002\"),\n",
    "]\n",
    " \n",
    "df = spark.createDataFrame(data, schema=schema)\n",
    "df = df.filter(~col(\"Quantity\").rlike('^[a-zA-Z]*$'))\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "a9126e14-fdcc-44e5-a4b8-53084ca1c5e0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------------------------------------------------------------+-----+\n|quote                                                                     |count|\n+--------------------------------------------------------------------------+-----+\n|Work hard in #silence, let #success make the noise.                       |2    |\n|Be #yourself; everyone else is already taken.                             |1    |\n|The only way to do #greatwork is to #love what you do.                    |2    |\n|#Believe you can and you're #halfway there.                               |2    |\n|The #future belongs to those who #believe in the #beauty of their #dreams.|4    |\n+--------------------------------------------------------------------------+-----+\n\n"
     ]
    }
   ],
   "source": [
    "# Databricks notebook source\n",
    "#Find out hashtag count for each quote\n",
    "\n",
    "from pyspark.sql.functions import col,split,size\n",
    "data = [\n",
    "    (\"Work hard in #silence, let #success make the noise.\",),\n",
    "    (\"Be #yourself; everyone else is already taken.\",),\n",
    "    (\"The only way to do #greatwork is to #love what you do.\",),\n",
    "    (\"#Believe you can and you're #halfway there.\",),\n",
    "    (\"The #future belongs to those who #believe in the #beauty of their #dreams.\",)\n",
    "]\n",
    "df = spark.createDataFrame(data, [\"quote\"])\n",
    "df = df.withColumn(\"count\", size(split(col(\"quote\"),\"#\"))-1)\n",
    "df.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "b644f57c-708f-431a-a9d4-cea5867ac0a1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----------------+------+\n|emp_id|            name|salary|\n+------+----------------+------+\n|  1003| Michael Johnson| 75000|\n|  6000|     Emma Wilson| 80000|\n|  2900|William Anderson| 68000|\n+------+----------------+------+\n\n"
     ]
    }
   ],
   "source": [
    "\"\"\" Write a PySpark program to select every 3rd (nth) row in the dataset \"\"\"\n",
    "from pyspark.sql.functions import row_number,lit\n",
    "from pyspark.sql.window import Window\n",
    "# Define schema for the DataFrame\n",
    "schema = StructType([\n",
    " StructField(\"emp_id\", IntegerType(), True),\n",
    " StructField(\"name\", StringType(), True),\n",
    " StructField(\"salary\", IntegerType(), True)\n",
    "])\n",
    "\n",
    "# Sample employee data\n",
    "data = [\n",
    " (1001, \"John Doe\", 50000),\n",
    " (2001, \"Jane Smith\", 60000),\n",
    " (1003, \"Michael Johnson\", 75000),\n",
    " (4000, \"Emily Davis\", 55000),\n",
    " (1005, \"Robert Brown\", 70000),\n",
    " (6000, \"Emma Wilson\", 80000),\n",
    " (1700, \"James Taylor\", 65000),\n",
    " (8000, \"Olivia Martinez\", 72000),\n",
    " (2900, \"William Anderson\", 68000),\n",
    " (3310, \"Sophia Garcia\", 67000)\n",
    "]\n",
    "\n",
    "# Create DataFrame\n",
    "df = spark.createDataFrame(data, schema)\n",
    "windowspec = Window.orderBy(lit(\"1\"))\n",
    "df = df.withColumn(\"rn\",row_number().over(windowspec)).filter(col(\"rn\")%3==0).drop(\"rn\")\n",
    "\n",
    "# Show DataFrame\n",
    "df.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "cad67acb-46cd-4d61-b394-847818a81cab",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---------+-----------+\n|product_id|sale_date|total_sales|\n+----------+---------+-----------+\n|         1|  2023-01|        200|\n|         1|  2023-02|        300|\n+----------+---------+-----------+\n\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Write a pyspark code to rank the products based on their total sales amount for each month, and return the top product for each month.\n",
    "\"\"\"\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.window import Window\n",
    "sales_data = [\n",
    " {\"product_id\": 1, \"sale_date\": \"2023-01-05\", \"amount\": 100},\n",
    " {\"product_id\": 2, \"sale_date\": \"2023-01-08\", \"amount\": 150},\n",
    " {\"product_id\": 1, \"sale_date\": \"2023-01-15\", \"amount\": 100},\n",
    " {\"product_id\": 3, \"sale_date\": \"2023-01-20\", \"amount\": 100},\n",
    " {\"product_id\": 2, \"sale_date\": \"2023-02-03\", \"amount\": 180},\n",
    " {\"product_id\": 3, \"sale_date\": \"2023-02-10\", \"amount\": 250},\n",
    " {\"product_id\": 1, \"sale_date\": \"2023-02-15\", \"amount\": 300},\n",
    "]\n",
    "schema=\"product_id int ,sale_date string,amount long\"\n",
    "\n",
    "df=spark.createDataFrame(sales_data,schema)\n",
    "df = df.withColumn(\"sale_date\",date_format(to_date(col(\"sale_date\")),'yyyy-MM'))\n",
    "df = df.groupBy(\"product_id\",\"sale_date\").agg(sum(\"amount\").alias(\"total_sales\"))\n",
    "window_spec = Window.partitionBy(\"sale_date\").orderBy(desc(\"total_sales\"))\n",
    "df = df.withColumn(\"rn\",row_number().over(window_spec)).filter(col(\"rn\")==1).drop(\"rn\")\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "3a728f36-751b-48a6-ba69-0edd3aaa0782",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----+------+---------+---+----+------+---------+\n| id|name|salary|managerId| id|name|salary|managerId|\n+---+----+------+---------+---+----+------+---------+\n|  1| Joe| 70000|        3|  3| Sam| 60000|     null|\n+---+----+------+---------+---+----+------+---------+\n\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\"\"\"SELF JOIN\"\"\"\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import col\n",
    "employees_data = [\n",
    "(1, 'Joe', 70000, 3),\n",
    "(2, 'Henry', 80000, 4),\n",
    "(3, 'Sam', 60000, None),\n",
    "(4, 'Max', 90000, None)\n",
    "]\n",
    "\n",
    "\n",
    "employees_schema = StructType([\n",
    "StructField(\"id\", IntegerType(), True),\n",
    "StructField(\"name\", StringType(), True),\n",
    "StructField(\"salary\", IntegerType(), True),\n",
    "StructField(\"managerId\", IntegerType(), True)])\n",
    "\n",
    "df_employee=spark.createDataFrame(employees_data,employees_schema)\n",
    "df_final=df_employee.alias(\"e1\").join(df_employee.alias(\"e2\"),col(\"e1.managerId\")==col(\"e2.id\"),\"inner\").filter(col(\"e1.salary\")>col(\"e2.salary\"))\n",
    "df_final.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "e5c1b207-c324-4c05-bc8a-d7362c5653c8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n|    Email|\n+---------+\n|abc@g.com|\n+---------+\n\n"
     ]
    }
   ],
   "source": [
    "\"\"\"FIND DUPS \"\"\"\n",
    "# Databricks notebook source\n",
    "from pyspark.sql.functions import col,count\n",
    "data = [\n",
    "(1, 'abc@g.com'),\n",
    "(2, 'xyz@g.com'),\n",
    "(3, 'abc@g.com' ),\n",
    "(4, 'pqr@g.com')\n",
    "]\n",
    "schema = \"id int,Email string\"\n",
    "df=spark.createDataFrame(data,schema)\n",
    "df_2 = df.groupBy(col(\"Email\")).agg(count(\"*\").alias(\"count\")).filter(col(\"count\")>1).drop(\"count\")\n",
    "df_2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "81d30150-f114-4507-aa9c-6e2f323b2fd8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+\n| id|\n+---+\n|  4|\n|  5|\n+---+\n\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\"\"\"FIND MISSING NUMBERS in DATA FREAME\"\"\"\n",
    "from pyspark.sql.functions import col,min,max\n",
    "data = [\n",
    " (1, ),\n",
    " (2,),\n",
    " (3,),\n",
    " (6,),\n",
    " (7,),\n",
    " (8,)]\n",
    "df = spark.createDataFrame(data).toDF(\"id\")\n",
    "df_min_max = df.agg(min(col(\"id\")),max(col(\"id\")))\n",
    "df_final=spark.range(df_min_max.collect()[0][0],df_min_max.collect()[0][1]+1)\n",
    "df_final = df_final.subtract(df)\n",
    "df_final.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "084cbc01-f472-454e-9d3b-df5a91769acd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+------------+\n|observation_id|      result|\n+--------------+------------+\n|           101|     9.87E-7|\n|           102|  5.54467E-5|\n|           103|5.0345678E-4|\n+--------------+------------+\n\n+--------------+------------+------------+\n|observation_id|      result|         NUM|\n+--------------+------------+------------+\n|           101|     9.87E-7|0.0000009870|\n|           102|  5.54467E-5|0.0000554467|\n|           103|5.0345678E-4|0.0005034568|\n+--------------+------------+------------+\n\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\"\"\"NUMBER FORMATTING\"\"\"\n",
    "df = spark.createDataFrame([(101, 0.000000987), (102, 0.0000554467), (103, 0.00050345678)], [\"observation_id\", \"result\"])\n",
    "df.show()\n",
    "\n",
    "from pyspark.sql.functions import col,format_number\n",
    "df = df.withColumn(\"NUM\",format_number(col(\"result\"),10))\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "8a45db7c-9af0-4d79-bf24-573c895d768e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+----------+\n|spark_partition_id|total_rows|\n+------------------+----------+\n|0                 |124999    |\n|1                 |125000    |\n|2                 |125000    |\n|3                 |125000    |\n|4                 |125000    |\n|5                 |125000    |\n|6                 |125000    |\n|7                 |125000    |\n+------------------+----------+\n\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "1. total number of partitions\n",
    "2. count of rows in each partitions\n",
    "\"\"\"\n",
    "from pyspark.sql.functions import col,spark_partition_id,count\n",
    "df=spark.range(1,1000000)\n",
    "df_final=df.withColumn(\"spark_partition_id\",spark_partition_id())\n",
    "df_final=df_final.groupBy(col(\"spark_partition_id\")).agg(count(\"*\").alias(\"total_rows\"))\n",
    "df_final.show(truncate=False)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "environmentMetadata": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 2
   },
   "notebookName": "pyspark",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
